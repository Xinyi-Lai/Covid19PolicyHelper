---
title: "Final Project Report"
date: 08/06/2020
author: "Chenyue Jiao (cjiao4), Yulin Li (yulinl2), Yan Sun (yansun5), Xiaojuan Wang (xiaojuan)"
output: 
  html_document: 
    toc: yes
    toc_depth: 1
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Introduction

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(readr)
dat = read_csv("new.csv")
```

Over the past decade, rising prices in China's housing market have been increasingly overwhelming to the young families trying to settle down in major cities in China. As a group of university students potentially facing this issue in our near future, we want to understand how much housing prices in a Chinese metropolis like Beijing can be predicted and explained by some visible factors in the market, for example, characteristics of a house itself. 

The dataset we use is extracted from [this page](https://www.kaggle.com/ruiqurm/lianjia) on Kaggle. All the data in this dataset was fetched by its author Qichen Liu from [this page](https://bj.lianjia.com/chengjiao) on the Chinese housing website [Lianjia.com](https://bj.lianjia.com).

The raw dataset has a total of `r nrow(dat)` observations of houses in Beijing. We will only focus on those built after year 2010. This dataset has `r ncol(dat)` variables. Among them, we will use `price` as the response variable, which represents the average price (in Chinese RMB) per square meter. As for the predictor variables, we plan to use the followings:

- `square`: the area of the house in square meters. numeric predictor.
- `livingRoom`: the number of living room. numeric predictor.
- `drawingRoom`: the number of drawing room. numeric predictor.
- `kitchen`: the number of kitchen. numeric predictor.
- `bathroom`: the number of bathroom. numeric predictor.
- `floor`: the floor that the house is on. numeric predictor. (this predictor has some messy code in it, which we will preprocess before using it.)
- `buildingType`: the type of building. categorical predictor, including tower (1), bungalow (2), combination of plate and tower (3), plate (4).
- `constructionTime`: the time of construction. numeric predictor.
- `renovationCondition`: the condition of renovation. categorical predictor, including other (1), rough (2), simplicity (3), hardcover (4).
- `buildingStructure`: the structure type of the building. categorical predictor, including unknown (1), mixed (2), brick and wood (3), brick and concrete (4), steel (5), steel-concrete composite (6).
- `ladderRatio`: the proportion between number of residents on the same floor and number of elevator of ladder. It describes how many ladders a resident have on average. numeric predictor.
- `elevator`: whether the building has elevators (1) or not (0). categorical predictor.
- `subway`: whether there is (1) subways nearby or not (0). categorical predictor.
- `district`: which district the house is located in Beijing. categorical predictor including 13 levels.

In view of what we have learned in this course, we set our goal as building a multiple regression model capable of making predictions of the housing price with hope that this model could also provide us with further insights into the housing market.

# Methods

## Data preparation

```{r, message=FALSE, warning=FALSE, eval=FALSE}
library(readr)
dat = read_csv("new.csv")
```

The raw dataset consists of `r nrow(dat)` observations. We first performed a series of cleaning procedures to prepare the data for modeling:

```{r, warning=FALSE}
# remove missing data, which is stored as "NA"
dat = na.omit(dat)
# get rid of the messy codes in the floor variable and coerce the results to numeric values
dat$floor = as.numeric(matrix(unlist(strsplit(dat$floor, " ")), ncol = 2, byrow = TRUE)[ , 2])
# remove the variables we are not interested in such as url and transaction id
dat = subset(dat, select = c(price, square, livingRoom, drawingRoom, kitchen, bathRoom, floor, buildingType, constructionTime, renovationCondition, buildingStructure, ladderRatio, elevator, subway, district))
# coerce the categorical predictors to factor variables
dat$buildingType = as.factor(dat$buildingType)
dat$renovationCondition = as.factor(dat$renovationCondition)
dat$buildingStructure = as.factor(dat$buildingStructure)
dat$elevator = as.factor(dat$elevator)
dat$subway = as.factor(dat$subway)
dat$district = as.factor(dat$district)
# coerce constructionTime to numeric variable (originally character type)
dat$constructionTime = as.numeric(dat$constructionTime)
# remove missing values again since some of the preceding steps resulting in new NA values
dat = na.omit(dat)
# extract data of houses built after 2010 and drop all redundant levels in the categorical predictors
dat = subset(dat, constructionTime > 2010)
dat = droplevels(dat)
```

After data preparation, all missing values were removed, and we only extracted the observations of houses which are built after year 2010. In addition, we only included the predictors we are interested in (as listed in the introduction section). This left us with `r nrow(dat)` observations of `r length(dat)` variables (1 response variable + `r length(dat) - 1` predictor variables).

```{r}
num_obs = nrow(dat) # total number of observations
num_trn = round(num_obs * 0.60) # number of observations for the training data

set.seed(42)
trn_idx = sample(num_obs, num_trn) # randomly generate the index for the training data
dat_trn = dat[trn_idx, ] # training data
dat_tst = dat[-trn_idx, ] # testing data
```

Since we want to find a good model for predicting house prices, we randomly split the remaining data into a training set (60% of the data with `r nrow(dat_trn)` observations) and a testing set (40% of the data with `r nrow(dat_tst)` observations). We fit models and do cross-validation based on the training data alone, and evaluate the models we choose using the testing data.

## Model selection

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }
  
  
</style>

<div class="superbigimage">
```{r, echo=FALSE, fig.width=15, fig.height=10}
pairs(dat_trn)
```


```{r, include=FALSE}
get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_bp_pval = function(model) {
  bptest(model)$p.value
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_pval = function(model) {
  shapiro.test(resid(model))$p.value
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model, is_log) {
  ifelse(is_log, sqrt(sum(((dat_trn$price - exp(fitted(model))) / (1 - hatvalues(model))
  ) ^ 2) / length(fitted(model))), sqrt(mean((
    resid(model) / (1 - hatvalues(model))
  ) ^ 2)))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

test_mod = function(model, is_log = FALSE){
  c(loocv_rmse = get_loocv_rmse(model, is_log), 
    adj_r2 = get_adj_r2(model), 
    bp_pval = get_bp_pval(model), 
    sw_pval = get_sw_pval(model), 
    num_params = get_num_params(model))
}

diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit){
    par(mfrow = c(1, 2), pty="s")
    
    plot(fitted(model), resid(model), col = "grey", pch = 20, 
         xlab = "Fitted", ylab = "Residual", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = "darkorange", lwd = 2)
    
    qqnorm(resid(model), col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit){
    list(p_val = shapiro.test(resid(model))$p, 
         decision = ifelse(test = shapiro.test(resid(model))$p < alpha, 
                           yes = "Reject", no = "Fail to Reject"))
  }
}

get_test_rmse = function(model) {
  sqrt(mean((dat_tst$price - predict(model, newdata = dat_tst))^ 2))
}

get_perc_err = function(model) {
  actual = dat_tst$price
  predicted = predict(model, dat_tst)
  100 * mean((abs(actual - predicted)) / actual)
}
```

# Results

The following plots show the fitted vs. residuals plots and the normal Q-Q plots for the four models we selected in the previous section.

```{r, echo=FALSE}
# Two models we selected
model_1 = lm(price ~ square + poly(ladderRatio, 2) + district + constructionTime + kitchen + bathRoom + buildingType + elevator + renovationCondition + subway + I(kitchen ^ 2) + I(livingRoom ^ 2) + kitchen:subway + buildingType:subway + elevator:subway + renovationCondition:subway + square:floor + square:constructionTime + square:ladderRatio + square:subway + livingRoom:kitchen + livingRoom:bathRoom +  livingRoom:ladderRatio +  constructionTime:district + square:ladderRatio:district, data = dat_trn)

model_1_rem = lm(price ~ square + poly(ladderRatio, 2) + district + constructionTime + kitchen + bathRoom + buildingType + elevator + renovationCondition + subway + I(kitchen ^ 2) + I(livingRoom ^ 2) + kitchen:subway + buildingType:subway + elevator:subway + renovationCondition:subway + square:floor + square:constructionTime + square:ladderRatio + square:subway + livingRoom:kitchen + livingRoom:bathRoom +  livingRoom:ladderRatio +  constructionTime:district + square:ladderRatio:district, data = dat_trn, subset = cooks.distance(model_1) <= 4 / nrow(dat_trn))

model_2 = lm(price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) + floor + livingRoom + drawingRoom + bathRoom + buildingType + constructionTime + elevator + subway + district + livingRoom:square + I(square ^ 2):drawingRoom + bathRoom:I(square ^ 2) + poly(square, 2, raw = TRUE):buildingType + poly(ladderRatio, 2, raw = TRUE):bathRoom + ladderRatio:drawingRoom + floor:ladderRatio + buildingType:ladderRatio + constructionTime:ladderRatio + livingRoom:subway + drawingRoom:floor + buildingType:subway +constructionTime:district, data = dat_trn)

model_2_rem = lm(price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) + floor + livingRoom + drawingRoom + bathRoom + buildingType + constructionTime + elevator + subway + district + livingRoom:square + I(square ^ 2):drawingRoom + bathRoom:I(square ^ 2) + poly(square, 2, raw = TRUE):buildingType + poly(ladderRatio, 2, raw = TRUE):bathRoom + ladderRatio:drawingRoom + floor:ladderRatio + buildingType:ladderRatio + constructionTime:ladderRatio + livingRoom:subway + drawingRoom:floor + buildingType:subway +constructionTime:district, data = dat_trn, subset = cooks.distance(model_2) <= 4 / nrow(dat_trn))
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 1: Model diagnostics plots for model_1"}
diagnostics(model_1, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 2: Model diagnostics plots for model_1_rem"}
diagnostics(model_1_rem, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 3: Model diagnostics plots for model_2"}
diagnostics(model_2, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 4: Model diagnostics plots for model_2_rem"}
diagnostics(model_2_rem, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

<br/>

The following table summarizes the values of different metrics for each model:

```{r, warning=FALSE, echo=FALSE}
sel_models = c("`model_1`", "`model_1_rem`", "`model_2`", "`model_2_rem`")
num_params = c(
  test_mod(model_1)["num_params"],
  test_mod(model_1_rem)["num_params"],
  test_mod(model_2)["num_params"],
  test_mod(model_2_rem)["num_params"]
)
Loocv_RMSE = c(
  test_mod(model_1)["loocv_rmse"],
  test_mod(model_1_rem)["loocv_rmse"],
  test_mod(model_2)["loocv_rmse"],
  test_mod(model_2_rem)["loocv_rmse"]
)
adjR2 = c(
  test_mod(model_1)["adj_r2"],
  test_mod(model_1_rem)["adj_r2"],
  test_mod(model_2)["adj_r2"],
  test_mod(model_2_rem)["adj_r2"]
)
test_RMSE = c(
  get_test_rmse(model_1),
  get_test_rmse(model_1_rem),
  get_test_rmse(model_2),
  get_test_rmse(model_2_rem)
)
perc_err = c(
  get_perc_err(model_1),
  get_perc_err(model_1_rem),
  get_perc_err(model_2),
  get_perc_err(model_2_rem)
)
results = data.frame(sel_models, num_params, Loocv_RMSE, adjR2, test_RMSE, perc_err)
colnames(results) = c("Model",
                      "No. of $\\beta$ Parameters",
                      "LOOCV-RMSE",
                      "Adjusted R-squared",
                      "Test RMSE",
                      "Average Percent Error")
knitr::kable(results, format = "markdown")
```

Since `model_1` has the lowest LOOCV-RMSE and `model_1_rem` has the lowest Average Percent Error, we prefer to choose these two models as our prediction model.

<br/>

The followings are the predicted vs. actual values plots for the two best models we selected based on Test RMSE and Average Percent Error.

```{r, echo=FALSE, fig.align="center"}
plot(predict(model_1, newdata = dat_tst) ~ dat_tst$price, col = "grey", pch = 20,
     xlab = "Actual Home Prices", ylab = "Predicted Home Prices", 
     main = "Predicted vs. actual plot for model_1")
abline(0, 1, col = "darkorange", lwd = 2)
```

```{r, echo=FALSE, fig.align="center"}
plot(predict(model_1_rem, newdata = dat_tst) ~ dat_tst$price, col = "grey", pch = 20,
     xlab = "Actual Home Prices", ylab = "Predicted Home Prices", 
     main = "Predicted vs. actual plot for model_1_rem")
abline(0, 1, col = "darkorange", lwd = 2)
```

# Discussion

##	Is hierarchy necessary for a predictive model (under the circumstance that a lower-degree term or a main effect is non-significant)? 

When fitting multiple regression models, we usually need to respect model hierarchy. This means that the main effects of predictors need to be included for their interaction to be included, and that lower-order predictors need to be included for their higher-order polynomials to be included. However, sometimes a single predictor may not affect the response in a significant way, while its interactions with other predictors do. Similarly, lower-order polynomial of a predictor may not be significant in model summary but the higher-order terms do. In such cases, do we need to respect model hierarchy when our goal is simply to use the model to make predictions?

We've learned from the textbook that for a model with the purpose of making predication, the only consideration that we need to focus on is how well the model fits the training data and can make accurate predictions on the testing data. The correlation and causation are not important since we do not care how to interpret the model. Therefore, it is interesting to ask whether hierarchy is necessary for a preditive model.

In this project, we used Test RMSE and Average Percent Error as the metrics to determine which model is better for prediction. We compared two models: `model_1` does not respect model hierarchy while `model_2` does. The results in the table in section 3 showed that the former gave a better (i.e., lower) Test RMSE and a better (i.e., lower) Average Percent Error (as well as better (i.e., lower) LOOCV-RMSE and better (i.e., higher) adjusted R-squared). This indicates that model hierarchy may not be that important for a predictive model.

##	Should we remove outliers when training the model?

Outliers are points which do not fit the model well. They can have large effects on the regression and thus are considered influential outliers. Often, the removal of influential outliers can help us fit a model that can better account for the effects of predictors. This is very important for a model for explanation. However, will it also be helpful to remove outliers when fitting a model for prediction?

In this project, we compared two pairs of models: `model_1` (with outliers) vs. `model_1_rem` (outliers removed) and `mode_2` (with outliers) vs. `model_2_rem` (outliers removed). From the table in section 3, we see that for the training data the two models with outliers removed did have lower LOOCV-RMSE values and higher adjusted-R-squared values compared to their corresponding models with outliears included. However, results are conflict regarding Test RMSE and Average Percent Error. For `model_1` and `model_1_rem` which ignore model hierarchy, the one with outliers excluded gave worse (i.e., higher) Test RMSE but better (i.e., lower) Average Percent Error. Nevertheless, the values between these two models are relatively small. On the other hand, for `model_2` and `model_2_rem` which respect model hierarchy, the one with outliers removed gave a much worse Test RMSE and a somewhat worse Average Percent Error. Therefore, we could not reach a firm conclusion about whether we should remove outliers when training the model solely based on the current project.

##	Which model(s) should we choose and how well does it (do they) work? 

In section 3, we reported the results of four candidate models we considered and compared their Test RMSE and Average Percent Error values. From the table in section 3, we see that `model_1` gave the best (i.e., lowest) Test RMSE whereas `model_1_rem` yielded the best (i.e., lowest) Average Percent Error. Since their differences are quite small, both models were chosen as our 'best' models. However, how well do they actually do in predicting house prices in Beijing on the testing data?

First, we see from the table that the calculated average percent error seems rather high at approximately 20% for both models. However, when we look at the two predicted versus actual values plots, we see many points are below the orange $y = x$ line. This means that for those data points the predicted prices are smaller than the actual values. That is, for those data points, the model underestimates the house prices. This is the most severe for extremely expensive houses, which contribute significantly to the overall percent error. We think this segguests the need for futher analysis, in particular, the need for a more flexible model than the current ones.

# Appendix

## Other models we ran but are not reported in the methods section

## Helper functions we made

```{r helper_functions, eval=FALSE}
get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_bp_pval = function(model) {
  bptest(model)$p.value
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_pval = function(model) {
  shapiro.test(resid(model))$p.value
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model, is_log) {
  ifelse(is_log, sqrt(sum(((dat_trn$price - exp(fitted(model))) / (1 - hatvalues(model))
  ) ^ 2) / length(fitted(model))), sqrt(mean((
    resid(model) / (1 - hatvalues(model))
  ) ^ 2)))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

test_mod = function(model, is_log = FALSE){
  c(loocv_rmse = get_loocv_rmse(model, is_log), 
    adj_r2 = get_adj_r2(model), 
    bp_pval = get_bp_pval(model), 
    sw_pval = get_sw_pval(model), 
    num_params = get_num_params(model))
}

diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit){
    par(mfrow = c(1, 2), pty="s")
    
    plot(fitted(model), resid(model), col = "grey", pch = 20, 
         xlab = "Fitted", ylab = "Residual", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = "darkorange", lwd = 2)
    
    qqnorm(resid(model), col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit){
    list(p_val = shapiro.test(resid(model))$p, 
         decision = ifelse(test = shapiro.test(resid(model))$p < alpha, 
                           yes = "Reject", no = "Fail to Reject"))
  }
}

get_test_rmse = function(model) {
  sqrt(mean((dat_tst$price - predict(model, newdata = dat_tst))^ 2))
}

get_perc_err = function(model) {
  actual = dat_tst$price
  predicted = predict(model, dat_tst)
  100 * mean((abs(actual - predicted)) / actual)
}
```


