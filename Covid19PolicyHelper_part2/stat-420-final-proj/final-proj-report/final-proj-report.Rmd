---
title: "Prediction of Housing Price in Beijing by Multiple Linear Regression Modeling"
date: 08/06/2020
author: "Chenyue Jiao (cjiao4), Yulin Li (yulinl2), Yan Sun (yansun5), Xiaojuan Wang (xiaojuan)"
output: 
  html_document: 
    theme: readable
    toc: yes
    toc_depth: 3
    number_sections: yes
# editor_options:
  # chunk_output_type: console
urlcolor: cyan
editor_options: 
  chunk_output_type: inline
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Introduction

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(readr)
dat = read_csv("new.csv")
```

Over the past decade, rising prices in China's housing market have been increasingly overwhelming to the young families trying to settle down in major cities in China. As a group of university students potentially facing this issue in our near future, we want to understand how much housing prices in a Chinese metropolis like Beijing can be predicted and explained by some visible factors in the market, for example, characteristics of a house itself. 

The dataset we use is extracted from [this page](https://www.kaggle.com/ruiqurm/lianjia) on Kaggle. All the data in this dataset was fetched by its author Qichen Liu from [this page](https://bj.lianjia.com/chengjiao) on the Chinese housing website [Lianjia.com](https://bj.lianjia.com).

The raw dataset has a total of `r nrow(dat)` observations of houses in Beijing. We will only focus on those built after year 2010. This dataset has `r ncol(dat)` variables. Among them, we will use `price` as the response variable, which represents the average price (in Chinese RMB) per square meter. As for the predictor variables, we plan to use the followings:

- `square`: the area of the house in square meters. numeric predictor.
- `livingRoom`: the number of living room. numeric predictor.
- `drawingRoom`: the number of drawing room. numeric predictor.
- `kitchen`: the number of kitchen. numeric predictor.
- `bathroom`: the number of bathroom. numeric predictor.
- `floor`: the floor that the house is on. numeric predictor. (this predictor has some messy code in it, which we will preprocess before using it.)
- `buildingType`: the type of building. categorical predictor, including tower (1), bungalow (2), combination of plate and tower (3), plate (4).
- `constructionTime`: the time of construction. numeric predictor.
- `renovationCondition`: the condition of renovation. categorical predictor, including other (1), rough (2), simplicity (3), hardcover (4).
- `buildingStructure`: the structure type of the building. categorical predictor, including unknown (1), mixed (2), brick and wood (3), brick and concrete (4), steel (5), steel-concrete composite (6).
- `ladderRatio`: the proportion between number of residents on the same floor and number of elevator of ladder. It describes how many ladders a resident have on average. numeric predictor.
- `elevator`: whether the building has elevators (1) or not (0). categorical predictor.
- `subway`: whether there is (1) subways nearby or not (0). categorical predictor.
- `district`: which district the house is located in Beijing. categorical predictor including 13 levels.

In view of what we have learned in this course, we set our goal as building a multiple regression model capable of making predictions of the housing price with hope that this model could also provide us with further insights into the housing market.

# Methods

## Data Preprocessing

```{r, message=FALSE, warning=FALSE, eval=FALSE}
library(readr)
dat = read_csv("new.csv")
```

The raw dataset consists of `r nrow(dat)` observations. We first performed data cleaning that removes all missing values and coercion of data types. 

```{r, warning=FALSE}
# remove missing data, which is stored as "NA"
dat = na.omit(dat)
# get rid of the messy codes in the floor variable and coerce the results to numeric values
dat$floor = as.numeric(matrix(unlist(strsplit(dat$floor, " ")), ncol = 2, byrow = TRUE)[ , 2])
# remove the variables we are not interested in such as url and transaction id
dat = subset(dat, select = c(price, square, livingRoom, drawingRoom, kitchen, bathRoom, floor, buildingType, constructionTime, renovationCondition, buildingStructure, ladderRatio, elevator, subway, district))
# coerce the categorical predictors to factor variables
dat$buildingType = as.factor(dat$buildingType)
dat$renovationCondition = as.factor(dat$renovationCondition)
dat$buildingStructure = as.factor(dat$buildingStructure)
dat$elevator = as.factor(dat$elevator)
dat$subway = as.factor(dat$subway)
dat$district = as.factor(dat$district)
# coerce constructionTime to numeric variable (originally character type)
dat$constructionTime = as.numeric(dat$constructionTime)
# remove missing values again since some of the preceding steps resulting in new NA values
dat = na.omit(dat)
# extract data of houses built after 2010 and drop all redundant levels in the categorical predictors
dat = subset(dat, constructionTime > 2010)
dat = droplevels(dat)
```

For simplicity, we only extracted the observations of houses which are built after year 2010. In addition, we only included the predictors we are interested in (as listed in the introduction). This left us with `r nrow(dat)` observations of `r length(dat)` variables (1 response variable + `r length(dat) - 1` predictor variables).

```{r}
num_obs = nrow(dat) # total number of observations
num_trn = round(num_obs * 0.60) # number of observations for the training data

set.seed(42)
trn_idx = sample(num_obs, num_trn) # randomly generate the index for the training data
dat_trn = dat[trn_idx, ] # training data
dat_tst = dat[-trn_idx, ] # testing data
```

Since we want to find a good model for predictive purpose, we randomly split the remaining data into a training set (60% of the data with `r nrow(dat_trn)` observations) and a testing set (40% of the data with `r nrow(dat_tst)` observations). In the following sections, we will fit different models and perform cross-validation based on the training data alone, and evaluate the models we choose using the testing data.

## Model Selection

```{r, include=FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_bp_pval = function(model) {
  bptest(model)$p.value
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_pval = function(model) {
  shapiro.test(resid(model))$p.value
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model, is_log) {
  ifelse(
    is_log, 
    sqrt(mean(na.omit(((dat_trn$price - exp(fitted(model))) / (1 - hatvalues(model))) ^ 2))),
    sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
  )
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

test_mod = function(model, is_log = FALSE){
  c(loocv_rmse = get_loocv_rmse(model, is_log), 
    adj_r2 = get_adj_r2(model), 
    bp_pval = get_bp_pval(model), 
    sw_pval = get_sw_pval(model), 
    num_params = get_num_params(model))
}

diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit){
    par(mfrow = c(1, 2), pty="s")
    
    plot(fitted(model), resid(model), col = "grey", pch = 20, 
         xlab = "Fitted", ylab = "Residual", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = "darkorange", lwd = 2)
    
    qqnorm(resid(model), col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit){
    list(p_val = shapiro.test(resid(model))$p, 
         decision = ifelse(test = shapiro.test(resid(model))$p < alpha, 
                           yes = "Reject", no = "Fail to Reject"))
  }
}

get_test_rmse = function(model) {
  sqrt(mean((dat_tst$price - predict(model, newdata = dat_tst))^ 2))
}

get_perc_err = function(model) {
  actual = dat_tst$price
  predicted = predict(model, dat_tst)
  100 * mean((abs(actual - predicted)) / actual)
}
```

### Starting Point (Model 0)
**Observations from the `pairs()` Plot; A Full Model (Model 0)**

To get a sense of the relationships between our variables of interests, we first look at the `pairs()` plot. 

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }
  
  
</style>

<div class="superbigimage">
```{r, echo=FALSE, fig.width=15, fig.height=10}
pairs(dat_trn, col = "dodgerblue")
```
</div>

Although the way this plot looks does not give us any clear clue, it seems that there could be some polynomial or logarithmic relationship between `price` and predictors `square` and `ladderRatio`. Also, the categorical predictor `district` distinguishes between high and low prices, which suggests that an interactive model is necessary. These observations somewhat match our common sense, and we will take this as a starting point for our models. 

Given the above information extracted from the `pairs()` plot, we attempt the following "full models" as well as their log-transformed versions: 

```{r}
# an additive model with all first-order terms
m1 = lm(price ~ ., data = dat_trn)
# an interactive model with all first-order terms and two-way interactions
m2 = lm(price ~ . ^ 2, data = dat_trn)
# a polynomial model with almost all first-order terms
m3 = lm(
  price ~ poly(square, 2) + poly(ladderRatio, 2) + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + ladderRatio + elevator + subway + district,
  data = dat_trn
)
# a polynomial model with almost all first-order terms and two-way interactions
m4 = lm(
  price ~ (poly(square, 2) + poly(ladderRatio, 2) + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + elevator + subway + district) ^ 2,
  data = dat_trn
)

# log-transformed versions:

# an additive model with all first-order terms (with a log-transformed repsonse)
m1_log = lm(log(price) ~ ., data = dat_trn)
# an interactive model with all first-order terms and two-way interactions (with a log-transformed repsonse)
m2_log = lm(log(price) ~ . ^ 2, data = dat_trn)
# a polynomial model with almost all first-order terms (with a log-transformed repsonse)
m3_log = lm(
  log(price) ~ poly(square, 2) + poly(ladderRatio, 2) + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + ladderRatio + elevator + subway + district,
  data = dat_trn
)
# a polynomial model with almost all first-order terms and two-way interactions (with a log-transformed repsonse)
m4_log = lm(
  log(price) ~ (poly(square, 2) + poly(ladderRatio, 2) + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + elevator + subway + district) ^ 2,
  data = dat_trn
)
```

```{r}
test_mod(m1)
test_mod(m1_log, is_log = TRUE)
test_mod(m2)
test_mod(m2_log, is_log = TRUE)
test_mod(m3)
test_mod(m3_log, is_log = TRUE)
test_mod(m4)
test_mod(m4_log, is_log = TRUE)
```

Comparing between these models by their test results as above, we found that the models with a log-transformed response usually perform poorer than their non-transformed version (in terms of Adjusted $R^2$). Therefore, we decide to stick with the non-transformed response throughout the rest of our project.

Among all "full models" with a non-transformed response, the interactive polynomial model `m4` fits the best (highest adjusted $R^2$). Therefore, we choose it as our starting model for backward selection and then attempt AIC to resolve its issue of over-fitting (high LOOCV-RMSE). 

```{r}
mod_full = m4
```

### Backward Selection Step 1 (Model 1)
**AIC Selection Starting from the Full Model (Model 1)**

Starting from the full model chosen above, we obtain a smaller model selected by the backward AIC procedure: 

```{r}
mod_aic_back = step(mod_full, trace = 0)
```

```{r}
test_mod(mod_aic_back)
```

The model size is reduced by $1/6$, but still large (around 300 parameters). Notably, the adjusted $R^2$ is slightly raised, which is desirable. However, the selected model still has the issue of having a high LOOCV-RMSE, suggesting that it is over-fitting. 

Moreover, as can be seen below, the selected model still involves considerably many non-significant coefficients, suggesting that some terms in this model are still redundant and thus can be removed. 

```{r}
head(summary(mod_aic_back)$coef, 20)
```

### Backward Selection Step 2 (Model 2)
**Manual Reduction of Model Size and Complexity (Model 2)**

Continuing from the result of the last step, we now try to manually reduce the size of the AIC-selected model until almost all coefficients left are significant. During this process, we tried removing coefficients one at a time such that the LOOCV-RMSE keeps dropping while the adjusted $R^2$ are kept unchanged or only slightly lowered, until the decrease of LOOCV-RMSE gradually slows down and finally terminates completely. 

Omitting the intermediate steps, we end with the following smaller model: 

```{r}
mod_small_back = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) + floor +
    livingRoom + drawingRoom + bathRoom + buildingType + constructionTime +
    elevator + subway + district + livingRoom:square + I(square ^ 2):drawingRoom +
    bathRoom:I(square ^ 2) + poly(square, 2, raw = TRUE):buildingType +
    poly(ladderRatio, 2, raw = TRUE):bathRoom + ladderRatio:drawingRoom +
    floor:ladderRatio + buildingType:ladderRatio + constructionTime:ladderRatio +
    livingRoom:subway + drawingRoom:floor + buildingType:subway +
    constructionTime:district,
  data = dat_trn)
```

```{r}
head(summary(mod_small_back)$coef, 20)
```

```{r}
test_mod(mod_small_back)
```

This time, the predictive power of our model, indicated by LOOCV-RMSE, is much better! Meanwhile, the size and complexity are also significantly lower (around 60 parameters) than the AIC selected model.

### Forward Selection Step 1 (Model 3)
**Manually Building an Intuitive Small Model (Model 3)**

Now, contrary to the preceding steps, we try another procedure: starting with a small model and going "forward". 

Combining observations from the `pairs()` plot and our commonsense plus some comparison between models, we decided a very small model as our starting point of forward selection: 

```{r}
mod_null = lm(price ~ square + poly(ladderRatio, 2) + district + constructionTime, data = dat_trn)
test_mod(mod_null)
```

This time, instead of immediately resorting to AIC selection procedure, we try building up the model manually first. The aim is to keep (almost) every coefficient left in the model significant, so that the remaining predictors are made the most use of. 

During this process, we first try adding the first-order terms one at a time based on an order determined by the importance of a predictor according to our daily experience and intuitions. Each time, only the predictors that are significant would be kept. 

After the first-order terms, we add some second-order terms and two-way interaction terms in a similar manner. 

Notably, the LOOCV-RMSE decreases gradually in this process, while the adjusted $R^2$ increases, though slower and slower. Finally, we stopped at the following model: 

```{r}
mod_small_forw = lm(
  price ~ square + poly(ladderRatio, 2) + district + constructionTime + 
  kitchen + bathRoom + 
  buildingType + elevator + renovationCondition + subway + 
  I(kitchen ^ 2) + I(livingRoom ^ 2) + 
  kitchen:subway + 
  buildingType:subway + 
  elevator:subway + 
  renovationCondition:subway + 
  square:floor + 
  square:constructionTime + 
  square:ladderRatio + 
  square:subway + 
  livingRoom:kitchen + 
  livingRoom:bathRoom + 
  livingRoom:ladderRatio + 
  constructionTime:district + 
  square:ladderRatio:district, 
data = dat_trn)
```

```{r}
head(summary(mod_small_forw)$coef, 20)
```


```{r}
test_mod(mod_small_forw)
```

The metrics of this model are roughly as good as (actually, even better!) than those of the model previously selected backwards by manual reduction of size (`mod_small_back`)! Meanwhile, the size and complexity are also significantly lower (around 60 parameters) than the AIC selected model.

### Forward Selection Step 2 (Model 4)
**AIC Selection Starting from the Small Model (Model 4)**

Now, we perform a forward AIC selection starting from the model we just obtained from the last step. 

```{r}
mod_aic_forw = step(
  mod_small_forw,
  direction = "forward",
  trace = 0,
  scope = price ~ (
    square + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + buildingStructure + elevator + subway + district + ladderRatio + I(kitchen ^ 2) + I(livingRoom ^ 2) + poly(ladderRatio, 2)
  ) ^ 2 + square:ladderRatio:district
) 
```

```{r}
head(summary(mod_aic_forw)$coef, 20)
```

```{r}
test_mod(mod_aic_forw)
```

Although the adjusted $R^2$ is notably raised, the LOOCV-RMSE of the selected model explodes again, and again a considerable portions of terms in the model are non-significant. This farmiliar situation again suggests that the model is having an issue of over-fitting.  

### Comparison between Model 1~4

Now we put together and compare between the metrics of the 4 models obtained as a result of each step above: 

```{r}
test_mod(mod_aic_back) # Model 1
test_mod(mod_small_back) # Model 2
test_mod(mod_small_forw) # Model 3
test_mod(mod_aic_forw) # Model 4
```

Clearly, the two smaller models (around 60 parameters) (Model 2 `mod_small_back` and Model 3 `mod_small_forw`) have significantly stronger prodictive power than the two large models (200~300 parameters) (Model 1 `mod_aic_back` and Model 4 `mod_aic_forw`), as indicated by the LOOCV-RMSE values. Meanwhile, though the adjusted $R^2$ of the two smaller models are lower than those of the two larger models, they are all above $0.59$ and are not too far from each other. 

Therefore, we decide the two smaller models as the best models we found. 

```{r}
# final selected models
model_1 = mod_small_forw 
model_2 = mod_small_back
```


### Outlier Removal

Combined with outlier removal, we obtained two versions of each small model we selected above. 

```{r}
# outlier removed version
model_1_rem = lm(
  price ~ square + poly(ladderRatio, 2) + district + constructionTime + kitchen + bathRoom + buildingType + elevator + renovationCondition + subway + I(kitchen ^ 2) + I(livingRoom ^ 2) + kitchen:subway + buildingType:subway + elevator:subway + renovationCondition:subway + square:floor + square:constructionTime + square:ladderRatio + square:subway + livingRoom:kitchen + livingRoom:bathRoom +  livingRoom:ladderRatio +  constructionTime:district + square:ladderRatio:district,
  data = dat_trn,
  subset = cooks.distance(model_1) <= 4 / nrow(dat_trn)
)

model_2_rem = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) + floor + livingRoom + drawingRoom + bathRoom + buildingType + constructionTime + elevator + subway + district + livingRoom:square + I(square ^ 2):drawingRoom + bathRoom:I(square ^ 2) + poly(square, 2, raw = TRUE):buildingType + poly(ladderRatio, 2, raw = TRUE):bathRoom + ladderRatio:drawingRoom + floor:ladderRatio + buildingType:ladderRatio + constructionTime:ladderRatio + livingRoom:subway + drawingRoom:floor + buildingType:subway +
    constructionTime:district,
  data = dat_trn,
  subset = cooks.distance(model_2) <= 4 / nrow(dat_trn)
)
```

Having in total four candidate models, we are now ready for final testing and comparison of the predictive power of these models using testing data. 

# Results

The following plots show the fitted vs. residuals plots and the normal Q-Q plots for the four models we selected in the previous section.

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 1: Model diagnostics plots for model_1"}
diagnostics(model_1, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 2: Model diagnostics plots for model_1_rem"}
diagnostics(model_1_rem, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 3: Model diagnostics plots for model_2"}
diagnostics(model_2, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

```{r, echo=FALSE, fig.align="center", fig.cap="Figure 4: Model diagnostics plots for model_2_rem"}
diagnostics(model_2_rem, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = FALSE)
```

<br/>

The following table summarizes the values of different metrics for each model:

```{r, warning=FALSE, echo=FALSE}
sel_models = c("`model_1`", "`model_1_rem`", "`model_2`", "`model_2_rem`")
num_params = c(
  test_mod(model_1)["num_params"],
  test_mod(model_1_rem)["num_params"],
  test_mod(model_2)["num_params"],
  test_mod(model_2_rem)["num_params"]
)
Loocv_RMSE = c(
  test_mod(model_1)["loocv_rmse"],
  test_mod(model_1_rem)["loocv_rmse"],
  test_mod(model_2)["loocv_rmse"],
  test_mod(model_2_rem)["loocv_rmse"]
)
adjR2 = c(
  test_mod(model_1)["adj_r2"],
  test_mod(model_1_rem)["adj_r2"],
  test_mod(model_2)["adj_r2"],
  test_mod(model_2_rem)["adj_r2"]
)
test_RMSE = c(
  get_test_rmse(model_1),
  get_test_rmse(model_1_rem),
  get_test_rmse(model_2),
  get_test_rmse(model_2_rem)
)
perc_err = c(
  get_perc_err(model_1),
  get_perc_err(model_1_rem),
  get_perc_err(model_2),
  get_perc_err(model_2_rem)
)
results = data.frame(sel_models, num_params, Loocv_RMSE, adjR2, test_RMSE, perc_err)
colnames(results) = c("Model",
                      "No. of $\\beta$ Parameters",
                      "LOOCV-RMSE",
                      "Adjusted R-squared",
                      "Test RMSE",
                      "Average Percent Error")
knitr::kable(results, format = "markdown")
```

Since `model_1` has the lowest LOOCV-RMSE and `model_1_rem` has the lowest Average Percent Error, we prefer to choose these two models as our prediction model.

<br/>

The followings are the predicted vs. actual values plots for the two best models we selected based on Test RMSE and Average Percent Error.

```{r, echo=FALSE, fig.align="center"}
plot(predict(model_1, newdata = dat_tst) ~ dat_tst$price, col = "grey", pch = 20,
     xlab = "Actual Home Prices", ylab = "Predicted Home Prices", 
     main = "Predicted vs. actual plot for model_1")
abline(0, 1, col = "darkorange", lwd = 2)
```

```{r, echo=FALSE, fig.align="center"}
plot(predict(model_1_rem, newdata = dat_tst) ~ dat_tst$price, col = "grey", pch = 20,
     xlab = "Actual Home Prices", ylab = "Predicted Home Prices", 
     main = "Predicted vs. actual plot for model_1_rem")
abline(0, 1, col = "darkorange", lwd = 2)
```

# Discussion

##	Is hierarchy necessary for a predictive model? 

When fitting multiple regression models, we usually need to respect model hierarchy. This means that the main effects of predictors need to be included for their interaction to be included, and that lower-order predictors need to be included for their higher-order polynomials to be included. However, sometimes a single predictor may not affect the response in a significant way, while its interactions with other predictors do. Similarly, lower-order polynomial of a predictor may not be significant in model summary but the higher-order terms do. In such cases, do we need to respect model hierarchy when our goal is simply to use the model to make predictions?

We've learned from the textbook that for a model with the purpose of making predication, the only consideration that we need to focus on is how well the model fits the training data and can make accurate predictions on the testing data. The correlation and causation are not important since we do not care how to interpret the model. Therefore, it is interesting to ask whether hierarchy is necessary for a preditive model.

In this project, we used Test RMSE and Average Percent Error as the metrics to determine which model is better for prediction. We compared two models: `model_1` does not respect model hierarchy while `model_2` does. The results in the table in section 3 showed that the former gave a better (i.e., lower) Test RMSE and a better (i.e., lower) Average Percent Error (as well as better (i.e., lower) LOOCV-RMSE and better (i.e., higher) adjusted R-squared. This indicates that model hierarchy may not be that important for a predictive model.

##	Should we remove outliers when training the model?

Outliers are points which do not fit the model well. They can have large effects on the regression and thus are considered influential outliers. Often, the removal of influential outliers can help us fit a model that can better account for the effects of predictors. This is very important for a model for explanation. However, will it also be helpful to remove outliers when fitting a model for prediction?

In this project, we compared two pairs of models: `model_1` (with outliers) vs. `model_1_rem` (outliers removed) and `model_2` (with outliers) vs. `model_2_rem` (outliers removed). From the table in section 3, we see that for the training data the two models with outliers removed did have lower LOOCV-RMSE values and higher adjusted-R-squared values compared to their corresponding models with outliears included. However, results are conflict regarding Test RMSE and Average Percent Error. For `model_1` and `model_1_rem` which ignore model hierarchy, the one with outliers excluded gave worse (i.e., higher) Test RMSE but better (i.e., lower) Average Percent Error. Nevertheless, the values between these two models are relatively small. On the other hand, for `model_2` and `model_2_rem` which respect model hierarchy, the one with outliers removed gave a much worse Test RMSE and a somewhat worse Average Percent Error. Therefore, we could not reach a firm conclusion about whether we should remove outliers when training the model solely based on the current project.

##	Which model(s) should we choose and how well does it (do they) work? 

In section 3, we reported the results of four candidate models we considered and compared their Test RMSE and Average Percent Error values. From the table in section 3, we see that `model_1` gave the best (i.e., lowest) Test RMSE whereas `model_1_rem` yielded the best (i.e., lowest) Average Percent Error. Since their differences are quite small, both models were chosen as our 'best' models. However, how well do they actually do in predicting house prices in Beijing on the testing data?

First, we see from the table that the calculated average percent error seems rather high at approximately 20% for both models. However, when we look at the two predicted versus actual values plots, we see many points are below the orange $y = x$ line. This means that for those data points the predicted prices are smaller than the actual values. That is, for those data points, the model underestimates the house prices. This is the most severe for extremely expensive houses, which contribute significantly to the overall percent error. We think this suggests the need for further analysis, in particular, the need for a more flexible model than the current ones.

# Appendix

## Omitted Intermediate Models We Tried

```{r, eval=FALSE}
m5 = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) +
    livingRoom + drawingRoom + bathRoom + buildingType + constructionTime + 
    elevator + subway + district + floor + square:livingRoom + 
    poly(square, 2, raw = TRUE):drawingRoom + I(square ^ 2):bathRoom + 
    I(square ^ 2):floor + poly(square, 2, raw = TRUE):buildingType + 
    I(square ^ 2):constructionTime + poly(ladderRatio, 2, raw = TRUE):bathRoom +
    poly(ladderRatio, 2, raw = TRUE):drawingRoom + ladderRatio:floor + 
    ladderRatio:buildingType + ladderRatio:constructionTime + livingRoom:floor + 
    livingRoom:subway + drawingRoom:floor + drawingRoom:constructionTime + 
    bathRoom:buildingType + floor:subway + buildingType:subway +
    constructionTime:elevator + constructionTime:district,
  data = dat_trn)

m6 = step(m5, trace = 0)

m7 = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) + floor +
    livingRoom + drawingRoom + bathRoom + buildingType + constructionTime +
    elevator + subway + district + livingRoom:square + I(square ^ 2):drawingRoom +
    bathRoom:I(square ^ 2) + poly(square, 2, raw = TRUE):buildingType +
    poly(ladderRatio, 2, raw = TRUE):bathRoom + ladderRatio:drawingRoom +
    floor:ladderRatio + buildingType:ladderRatio + constructionTime:ladderRatio +
    livingRoom:subway + drawingRoom:floor + buildingType:subway +
    constructionTime:district,
  data = dat_trn)

m8 = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) + floor +
    livingRoom + drawingRoom + bathRoom + buildingType + constructionTime +
    elevator + subway + district + livingRoom:square + I(square ^ 2):drawingRoom +
    bathRoom:I(square ^ 2) + poly(square, 2, raw = TRUE):buildingType +
    poly(ladderRatio, 2, raw = TRUE):bathRoom + ladderRatio:drawingRoom +
    floor:ladderRatio + buildingType:ladderRatio + constructionTime:ladderRatio +
    livingRoom:subway + drawingRoom:floor + buildingType:subway +
    constructionTime:district,
  data = dat_trn, subset = cooks.distance(m8) <= 4 / nrow(dat_trn))

m9 = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) +
    livingRoom + bathRoom + buildingType + constructionTime +
    elevator + subway + district + livingRoom:square + bathRoom:I(square ^ 2) +
    poly(square, 2, raw = TRUE):buildingType +
    poly(ladderRatio, 2, raw = TRUE):bathRoom + buildingType:ladderRatio +
    constructionTime:ladderRatio + livingRoom:subway + buildingType:subway +
    constructionTime:district,
  data = dat_trn)

m10 = lm(
  price ~ poly(square, 2, raw = TRUE) * poly(ladderRatio, 2, raw = TRUE) +
    livingRoom + bathRoom + buildingType + constructionTime +
    elevator + subway + district + livingRoom:square + bathRoom:I(square ^ 2) +
    poly(square, 2, raw = TRUE):buildingType +
    poly(ladderRatio, 2, raw = TRUE):bathRoom + buildingType:ladderRatio +
    constructionTime:ladderRatio + livingRoom:subway + buildingType:subway +
    constructionTime:district,
  data = dat_trn, subset = cooks.distance(m10) <= 4 / nrow(dat_trn))

m11 = step(lm(price ~ 1, data = dat_trn), scope = price ~ (poly(square, 2) + poly(ladderRatio, 2) + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + elevator + subway + district) ^ 2, direction = "forward", trace = 0)

m12 = step(lm(price ~ 1, data = dat_trn), scope = price ~ (square + I(square ^ 2) + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + ladderRatio + I(ladderRatio ^ 2) + elevator + subway + district) ^ 2, direction = "forward")

m13 = step(lm(price ~ 1, data = dat_trn), scope = price ~ (square + livingRoom + drawingRoom + kitchen + bathRoom + floor + buildingType + constructionTime + renovationCondition + ladderRatio + elevator + subway + district) ^ 2, direction = "forward")
```

## Helper Functions 

```{r helper_functions, eval=FALSE}
get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_bp_pval = function(model) {
  bptest(model)$p.value
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_pval = function(model) {
  shapiro.test(resid(model))$p.value
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model, is_log) {
  ifelse(
    is_log, 
    sqrt(mean(na.omit(((dat_trn$price - exp(fitted(model))) / (1 - hatvalues(model))) ^ 2))),
    sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
  )
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

test_mod = function(model, is_log = FALSE){
  c(loocv_rmse = get_loocv_rmse(model, is_log), 
    adj_r2 = get_adj_r2(model), 
    bp_pval = get_bp_pval(model), 
    sw_pval = get_sw_pval(model), 
    num_params = get_num_params(model))
}

diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE){
  if (plotit){
    par(mfrow = c(1, 2), pty="s")
    
    plot(fitted(model), resid(model), col = "grey", pch = 20, 
         xlab = "Fitted", ylab = "Residual", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = "darkorange", lwd = 2)
    
    qqnorm(resid(model), col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit){
    list(p_val = shapiro.test(resid(model))$p, 
         decision = ifelse(test = shapiro.test(resid(model))$p < alpha, 
                           yes = "Reject", no = "Fail to Reject"))
  }
}

get_test_rmse = function(model) {
  sqrt(mean((dat_tst$price - predict(model, newdata = dat_tst))^ 2))
}

get_perc_err = function(model) {
  actual = dat_tst$price
  predicted = predict(model, dat_tst)
  100 * mean((abs(actual - predicted)) / actual)
}
```


